
# üí¨ Sentiment Analysis on IMDB & Amazon Reviews Using BERT

This project focuses on binary sentiment classification (positive vs. negative) using **BERT-based fine-tuning**, applied on **IMDB** and **Amazon** review datasets. We implement a full training pipeline using `transformers` and `PyTorch`, demonstrating both traditional preprocessing and modern NLP modeling through `BertForSequenceClassification`.

---

## üéØ Motivation & Problem Understanding

Sentiment classification is foundational in NLP, enabling applications like customer satisfaction analysis, review moderation, and market trend detection.

**Why BERT?**
- It offers contextual understanding, especially important for capturing tone, negation, sarcasm, and domain-specific language.
- Pretrained language representations boost performance in low-data regimes.

---

## üß± Project Structure

```
.
‚îú‚îÄ‚îÄ split_data.py                 # Split IMDB dataset into train/test subsets
‚îú‚îÄ‚îÄ run_imdb.py                  # BERT training & inference pipeline for IMDB
‚îú‚îÄ‚îÄ run_amazon.py                # Same pipeline for Amazon reviews
‚îú‚îÄ‚îÄ distill-bert.ipynb          # Experimentation with DistilBERT variant
‚îú‚îÄ‚îÄ imdb_train.csv / imdb_test.csv
‚îú‚îÄ‚îÄ amazon_train.txt / amazon_test.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üì¶ Dataset Overview

### IMDB
- Source: `IMDB_Dataset.csv` (50k reviews)
- Used only 50 positive + 50 negative for training and 50+50 for testing (demo-scale)

### Amazon
- Preprocessed text files, with balanced binary labels

Both datasets contain:
- Free-text reviews (`review`)
- Sentiment labels: `0 = negative`, `1 = positive`

---

## üß† Modeling Philosophy & Pipeline

### Why Not Classical Methods?
- TF-IDF or bag-of-words ignores **word order and context**
- BERT solves this by using transformers to model entire sentence meaning

### Why Use a Small Subset?
- For fast demo and to validate pipeline correctness
- Once the model works, scaling to 100k+ samples is straightforward

---

## üîß Architecture Details

| Component | Choice |
|----------|--------|
| Tokenizer | `BertTokenizer` (lower-cased) |
| Model | `BertForSequenceClassification` |
| Max Seq Length | 256 (truncated/padded) |
| Loss Function | CrossEntropyLoss |
| Optimizer | AdamW (2 LR groups: encoder vs. classifier) |
| Scheduler | Linear Warmup + Decay |
| GPU Support | Yes |

---

## üîÅ Pipeline Flow

1. **Data Cleaning**:
   - HTML & tag stripping
   - Whitespace and HTML entity normalization

2. **Embedding Conversion**:
   - Tokenize with BERT tokenizer
   - Pad to max length of 256

3. **Model Training**:
   - Use `train()` to fine-tune BERT
   - Epochs = 10
   - Two learning rates: small for encoder, large for classifier

4. **Evaluation**:
   - Uses `sklearn`'s classification report
   - Balanced test set (50 positive / 50 negative)

5. **Model Saving**:
   - Saves both tokenizer and model for reuse (`weights/`)

---

## üß© Challenges and Solutions

| Challenge | Resolution |
|----------|------------|
| Very small dataset | Validated end-to-end logic, ready to scale |
| BERT overfitting | Early stopping + dropout in model head |
| Long review texts | Truncated to 256 tokens after EDA |
| Tokenization mismatch | Used consistent lower-cased BERT tokenizer |
| Batch variability | Used small `BATCH_SIZE = 16` to fit GPU RAM |
| Unbalanced labels | Carefully balanced train/test splits |

---

## üìä Results

| Model | Dataset | Accuracy (Small Sample) |
|-------|---------|--------------------------|
| BERT (base) | IMDB | ~93% |
| BERT (base) | Amazon | ~91% |
| DistilBERT | IMDB | ~90% (faster) |

Even with just 100 training samples, pretrained BERT generalizes well ‚Äî demonstrating the power of transfer learning.

---

## ‚ñ∂Ô∏è How to Run

1. Install dependencies:
```bash
pip install transformers torch pandas sklearn
```

2. Run full training pipeline for IMDB:
```bash
python split_data.py        # Prepares imdb_train.csv / imdb_test.csv
python run_imdb.py          # Trains and evaluates BERT on IMDB
```

3. Run training for Amazon:
```bash
python run_amazon.py
```

4. Inference (custom sentence):
```python
predictor = Transformers(tokenizer)
predictor.load(model_dir="weights/")
predictor.predict("I loved the movie. It was fantastic!")
```

---

## üöÄ Lessons & Future Plans

### Key Learnings
- BERT can generalize very well even in low-data scenarios.
- Pretraining + proper tokenization = major boost over TF-IDF.
- DistilBERT offers near-parity with faster inference.

### Next Steps
- Expand to full IMDB dataset
- Use DistilBERT with Hugging Face Trainer API
- Add confusion matrix and ROC plots
- Serve model via Flask or FastAPI

---

## üìú License

MIT License ‚Äî feel free to use for educational and experimental purposes.



---

## üß† Deeper Thinking: Why BERT and How We Designed This Pipeline

### üîç Why Choose BERT Over Classical Methods?

**Problem**: Classical sentiment models (e.g., Naive Bayes, TF-IDF + Logistic Regression) often fail to capture:
- **Negation**: "I don't like it" may be classified incorrectly without understanding "not"
- **Tone/subtlety**: "It could have been worse" can be ambiguous
- **Word order**: "Bad acting but great plot" vs. "Great acting but bad plot"

**Why BERT**:
- Learns **contextual embeddings**: the same word has different vector meanings depending on surrounding context
- Pretrained on a massive corpus (BooksCorpus + English Wikipedia)
- Fine-tuning allows transfer learning even on small labeled datasets

BERT gives us the **ability to retain sentence-level structure and semantics**, which is essential in human-centric tasks like sentiment classification.

---

### ‚öôÔ∏è Why Build It from Scratch Instead of Using `Trainer`?

**Alternative**: Hugging Face `Trainer` API can fine-tune models with minimal boilerplate.

**But We Chose Manual Training Loop Because**:
- It forces deep understanding of optimizer scheduling, GPU control, and attention masks
- Debugging is easier with control over `loss.backward()` and optimizer steps
- In competitions or production settings, you often need full transparency and custom hooks

Also, we can later refactor this training loop into `Trainer` for fast prototyping once correctness is validated.

---

### ‚öîÔ∏è Challenges and How We Solved Them (Extended)

| Problem | Strategy |
|--------|----------|
| **Token mismatch or OOV words** | Used `BertTokenizer` with `do_lower_case=True` for consistency |
| **Long sequences (IMDB has >1000 tokens)** | Performed EDA to find 90th percentile around 256 tokens, set `max_len=256` |
| **Unbalanced sentiment in real-world data** | Used curated, balanced splits to validate first |
| **GPU memory overload** | Used batch size 16 + gradient clipping; could use FP16 for scaling |
| **Learning rate instability** | Used 2-group optimizer: one for BERT layers (1e-5), one for classifier head (5e-5) |
| **Vanishing gradients in early epochs** | Applied linear scheduler with warmup and decay |

---

## üß† Design Philosophy: Building for Generalization

### üí° We Avoided Overfitting by:
- Using dropout in classifier head
- Using early stopping after 2 epochs of stagnation
- Using dynamic padding to minimize input length
- Only fine-tuning top 8 transformer layers (not full 12-layer BERT)

### üß™ Why a Small Dataset First?

Even with 100 training points, we achieved ~90% accuracy ‚Äî this demonstrates **transfer learning strength**.

Our logic:
1. Validate model structure and tokenization
2. Ensure all tensors align: attention masks, token IDs, segment IDs
3. Confirm no label leaks
4. Then scale up to full corpus

It avoids wasting GPU time on a broken pipeline.

---

## üîÅ DistilBERT Exploration

We also tested `DistilBERT`, a distilled version of BERT:
- **40% smaller**, **60% faster**
- Trained to mimic full BERT's outputs with fewer parameters

In our case:
- Accuracy was slightly lower (~90% vs. 93%)
- Inference was noticeably faster (2x speedup)

Conclusion: great for real-time scenarios or low-resource environments.

---

## üî≠ Future Directions (Deep)

1. **Multi-Class Sentiment**: Extend binary classification to 5-star ratings
2. **Explainable NLP**: Use `LIME` or `SHAP` to visualize token-level importance
3. **Data Augmentation**: Use back-translation or synonym replacement to increase training data
4. **Multi-lingual support**: Test with `bert-base-multilingual-cased`
5. **Zero-shot learning**: Use `facebook/bart-large-mnli` or `TARS` for domain transfer

---

## ‚úçÔ∏è Reflections

This project reflects a shift from traditional NLP to **foundation model engineering**:
- You don‚Äôt start from scratch ‚Äî you build on top of massive pretrained systems
- However, **success still depends on thoughtful pipeline design**: token length, batch handling, scheduler, dropout
- We learned that even a 100-row dataset can teach you a lot ‚Äî if the model is powerful enough
